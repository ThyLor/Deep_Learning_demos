{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import linalg\n",
    "import random as rn\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(list(self.word2vec.values()))\n",
    "            \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}        \n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        # New variable \n",
    "        self.list_words = list(self.word2vec.keys())\n",
    "        \n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        \n",
    "        words_and_score = []\n",
    "        pivot = self.word2vec[w]\n",
    "        \n",
    "        for i in range(len(self.embeddings)):\n",
    "            tmp_val = self.embeddings[i]\n",
    "            words_and_score.append([self.list_words[i], Word2vec.score_1(pivot,tmp_val)])\n",
    "            \n",
    "        words_and_score.sort(key = lambda x : x[1], reverse = True)\n",
    "        \n",
    "        top_K_words = []\n",
    "        \n",
    "        for j in range(K):\n",
    "            top_K_words.append(words_and_score[j+1][0])\n",
    "        \n",
    "        return top_K_words\n",
    "    \n",
    "    \n",
    "    def KNN(self,pivot, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        words_and_score = []\n",
    "        \n",
    "        for i in range(len(self.embeddings)):\n",
    "            tmp_val = self.embeddings[i]\n",
    "            words_and_score.append([self.list_words[i], Word2vec.score_1(pivot, tmp_val)])\n",
    "            \n",
    "        words_and_score.sort(key = lambda x : x[1], reverse = True)\n",
    "        \n",
    "        top_K_words = []\n",
    "        \n",
    "        for j in range(K):\n",
    "            top_K_words.append(words_and_score[j+1][0])\n",
    "            \n",
    "        return top_K_words\n",
    "                        \n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        \n",
    "        w1_vec = self.word2vec[w1]\n",
    "        w2_vec = self.word2vec[w2]\n",
    "        \n",
    "        return np.dot(w1_vec,w2_vec)/(np.linalg.norm(w1_vec) * np.linalg.norm(w2_vec))\n",
    "    \n",
    "    def score_1(w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        \n",
    "        return np.dot(w1,w2)/(np.linalg.norm(w1) * np.linalg.norm(w2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052404\n",
      "paris france 0.7775108541288563\n",
      "germany berlin 0.7420295235998394\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "['france', 'Paris', 'london', 'berlin', 'tokyo']\n",
      "['austria', 'europe', 'german', 'berlin', 'poland']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help      \n",
    "                \n",
    "        sentence_emb = []\n",
    "\n",
    "        if idf is False:\n",
    "            for sentence in sentences:\n",
    "                admitted_words = []\n",
    "                for w in sentence:\n",
    "                    try:\n",
    "                        admitted_words.append(self.w2v.word2vec[w])\n",
    "                    except:\n",
    "                        pass\n",
    "                if(len(admitted_words) == 0):\n",
    "                    sentence_emb.append([0]*300)\n",
    "                else:\n",
    "                    sentence_emb.append(np.sum([wv for wv in admitted_words], axis = 0)/len(admitted_words))\n",
    "                \n",
    "        else:\n",
    "            for sentence in sentences:\n",
    "                admitted_words = []\n",
    "                for w in sentence:\n",
    "                    try:\n",
    "                        admitted_words.append( (w, self.w2v.word2vec[w]) )\n",
    "                    except:\n",
    "                        pass\n",
    "                if(len(admitted_words) == 0):\n",
    "                    sentence_emb.append([0]*300)\n",
    "                else:\n",
    "                    normalization = np.sum([idf[w[0]] for w in admitted_words])\n",
    "                    sentence_emb.append(np.sum([ [float(idf[w[0]])*component for component in w[1]] for w in admitted_words],axis = 0)/normalization)\n",
    "                \n",
    "        return np.vstack(sentence_emb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        s_encode = self.encode([s], idf)\n",
    "        \n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        most_similar_words = []\n",
    "        \n",
    "        for i in range(len(keys)):\n",
    "            tmp_val = keys[i]\n",
    "            most_similar_words.append((sentences[i], np.dot(s_encode,tmp_val)/(np.linalg.norm(s_encode) * np.linalg.norm(tmp_val))))\n",
    "        \n",
    "        most_similar_words.sort(key = lambda x : x[1], reverse = True)\n",
    "            \n",
    "        #How to get the sentence for a given vector\n",
    "        for i in range(K):\n",
    "            print(most_similar_words[i+1][0])\n",
    "\n",
    "        return most_similar_words[1:K+1][0]\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        v1 = self.encode([s1],idf)\n",
    "        v2 = self.encode([s2],idf)\n",
    "        \n",
    "        return np.dot(v1,v2.T)/(np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        \n",
    "        # Decleare the idf dictionary\n",
    "        idf = {}\n",
    "        # Normalized dictionary\n",
    "        n_idf = {}\n",
    "        \n",
    "        for sent in sentences:\n",
    "             for w in set(sent): \n",
    "                idf[w] = idf.get(w, 0) + 1 \n",
    "                \n",
    "        for word in list(idf.keys()):\n",
    "            n_idf[word] = max(1, np.log10(len(sentences) / (idf[word])))\n",
    "            \n",
    "        return n_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'smiling', 'african', 'american', 'boy']\n",
      "['an', 'african', 'american', 'man', 'smiling']\n",
      "['a', 'little', 'african', 'american', 'boy', 'and', 'girl', 'looking', 'up']\n",
      "['african', 'american', 'woman', 'bouncing', 'black', 'basketball']\n",
      "['an', 'afican', 'american', 'woman', 'standing', 'behind', 'two', 'small', 'african', 'american', 'children']\n",
      "['a', 'girl', 'in', 'black', 'hat', 'holding', 'an', 'african', 'american', 'baby']\n",
      "['an', 'african', 'american', 'man', 'smiling']\n",
      "['an', 'african', 'american', 'man', 'is', 'sitting']\n",
      "['a', 'little', 'african', 'american', 'boy', 'and', 'girl', 'looking', 'up']\n",
      "['an', 'afican', 'american', 'woman', 'standing', 'behind', 'two', 'small', 'african', 'american', 'children']\n",
      "['a', 'girl', 'in', 'black', 'hat', 'holding', 'an', 'african', 'american', 'baby']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.45262547]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=250000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "file_sentences = \"sentences.txt\"\n",
    "\n",
    "f = open(file_sentences,'r')\n",
    "data = f.read().split('\\n')\n",
    "exclude = set(string.punctuation)\n",
    "data_cleaned = [(''.join(ch for ch in line if ch not in exclude)) for line in data]\n",
    "sentences = [word_tokenize(line) for line in data_cleaned if (len(line)>0)]\n",
    "f.close()\n",
    "    \n",
    "# Build idf scores for each word\n",
    "idf = {} if True else s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('' if not sentences else sentences[10])\n",
    "\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "idf = s2v.build_idf(sentences)\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "# DOWNLOAD\n",
    "# LOAD\n",
    "n_words = 50000\n",
    "w2v_en = Word2vec('wiki.en.vec',n_words)\n",
    "w2v_fr = Word2vec('wiki.fr.vec',n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "words_en = set(w2v_en.list_words)\n",
    "words_fr = set(w2v_fr.list_words)\n",
    "words_common = words_en.intersection(words_fr)\n",
    "\n",
    "n_words_common = len(words_common)\n",
    "n_words_distinct = n_words - n_words_common\n",
    "\n",
    "lst_en_words_distinct = list(words_en.difference(words_common))\n",
    "lst_fr_words_distinct = list(words_fr.difference(words_common))\n",
    "lst_words_common = list(words_common)\n",
    "\n",
    "d = len(w2v_en.word2vec[lst_en_words_distinct[0]])\n",
    "\n",
    "X = np.zeros((d,n_words_common))\n",
    "Y = np.zeros((d,n_words_common))\n",
    "\n",
    "for i in range(n_words_common):\n",
    "    X[:,i] = w2v_fr.word2vec[lst_words_common[i]]\n",
    "    Y[:,i] = w2v_en.word2vec[lst_words_common[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "U, s, Vh = linalg.svd(Y.dot(X.T))\n",
    "W = U.dot(Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Pick a french word and give me the top K translations **\n",
      "French word: fmi\n",
      "Top K english-related words are: \n",
      "-> macroeconomic\n",
      "-> monetary\n",
      "-> eurozone\n",
      "-> opec\n",
      "-> oecd\n",
      "French word: fermeture\n",
      "Top K english-related words are: \n",
      "-> reopened\n",
      "-> reopen\n",
      "-> closure\n",
      "-> closing\n",
      "-> closures\n",
      "French word: angulaire\n",
      "Top K english-related words are: \n",
      "-> perpendicular\n",
      "-> curvature\n",
      "-> amplitude\n",
      "-> dimensionless\n",
      "-> velocity\n",
      "French word: monnet\n",
      "Top K english-related words are: \n",
      "-> moreau\n",
      "-> leclerc\n",
      "-> mitterrand\n",
      "-> jean\n",
      "-> françois\n",
      "French word: lavedan\n",
      "Top K english-related words are: \n",
      "-> foix\n",
      "-> gascony\n",
      "-> aveyron\n",
      "-> navarre\n",
      "-> occitan\n",
      "French word: oricon\n",
      "Top K english-related words are: \n",
      "-> heatseekers\n",
      "-> chart\n",
      "-> billboard\n",
      "-> r&b/hip\n",
      "-> peaking\n",
      "French word: longueuil\n",
      "Top K english-related words are: \n",
      "-> quebec\n",
      "-> sherbrooke\n",
      "-> gatineau\n",
      "-> rivières\n",
      "-> montreal\n",
      "French word: agrégée\n",
      "Top K english-related words are: \n",
      "-> normale\n",
      "-> école\n",
      "-> sorbonne\n",
      "-> ecole\n",
      "-> doctorate\n",
      "French word: océanienne\n",
      "Top K english-related words are: \n",
      "-> austronesian\n",
      "-> polynesian\n",
      "-> oceania\n",
      "-> polynesia\n",
      "-> vanuatu\n",
      "French word: aiea\n",
      "Top K english-related words are: \n",
      "-> radiological\n",
      "-> multilateral\n",
      "-> reactors\n",
      "-> nuclear\n",
      "-> reactor\n",
      "** Pick a english word and give me the top K translations **\n",
      "English word: brooker\n",
      "Top K french-related words are: \n",
      "-> nicholls\n",
      "-> marsh\n",
      "-> cutler\n",
      "-> maloney\n",
      "-> davies\n",
      "English word: diagnosis\n",
      "Top K french-related words are: \n",
      "-> diagnostique\n",
      "-> diagnostics\n",
      "-> pathologies\n",
      "-> biopsie\n",
      "-> diagnostiqué\n",
      "English word: posner\n",
      "Top K french-related words are: \n",
      "-> lerner\n",
      "-> friedman\n",
      "-> goldstein\n",
      "-> oppenheimer\n",
      "-> cohen\n",
      "English word: emotion\n",
      "Top K french-related words are: \n",
      "-> émotionnelle\n",
      "-> émotion\n",
      "-> émotionnel\n",
      "-> imagination\n",
      "-> empathie\n",
      "English word: tiwari\n",
      "Top K french-related words are: \n",
      "-> sanjay\n",
      "-> singh\n",
      "-> kapoor\n",
      "-> chopra\n",
      "-> sharma\n",
      "English word: mandalay\n",
      "Top K french-related words are: \n",
      "-> phnom\n",
      "-> vientiane\n",
      "-> birmans\n",
      "-> thaïlandais\n",
      "-> thaï\n",
      "English word: prabhakar\n",
      "Top K french-related words are: \n",
      "-> sanjay\n",
      "-> shankar\n",
      "-> vijay\n",
      "-> kumar\n",
      "-> sharma\n",
      "English word: reconciling\n",
      "Top K french-related words are: \n",
      "-> réconciliation\n",
      "-> coexistence\n",
      "-> réconcilie\n",
      "-> conciliation\n",
      "-> contradictions\n",
      "English word: oriente\n",
      "Top K french-related words are: \n",
      "-> misiones\n",
      "-> universidad\n",
      "-> alianza\n",
      "-> centro\n",
      "-> oeste\n",
      "English word: derive\n",
      "Top K french-related words are: \n",
      "-> dérivent\n",
      "-> dériver\n",
      "-> dérivant\n",
      "-> proviendrait\n",
      "-> découle\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "K = 5\n",
    "n_showed_words = 10\n",
    "\n",
    "print(\"** Pick a french word and give me the top K translations **\")\n",
    "# Pick a french word and give me the top K translations\n",
    "for j in range(n_showed_words):\n",
    "    my_index = np.random.randint(n_words)\n",
    "    my_word = w2v_fr.list_words[my_index]\n",
    "    print(\"French word: \"+my_word)\n",
    "    related_words = w2v_en.KNN(W.dot(w2v_fr.word2vec[my_word]))\n",
    "    print(\"Top K english-related words are: \")\n",
    "    for l in range(K):\n",
    "        print(\"-> \"+related_words[l])\n",
    "print(\"** Pick a english word and give me the top K translations **\")\n",
    "# Pick an english word and give me the top K translations\n",
    "for j in range(n_showed_words):\n",
    "    my_index = np.random.randint(n_words)\n",
    "    my_word = w2v_en.list_words[my_index]\n",
    "    print(\"English word: \"+my_word)\n",
    "    related_words = w2v_fr.KNN((W.T).dot(w2v_en.word2vec[my_word]))\n",
    "    print(\"Top K french-related words are: \")\n",
    "    for l in range(K):\n",
    "        print(\"-> \"+related_words[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "def data_pp_1(fname,case):\n",
    "    f = open(fname,'r')\n",
    "    data = f.read().split('\\n')\n",
    "    exclude = set(string.punctuation)\n",
    "    data_cleaned = [ (''.join(ch for ch in line if ch not in exclude)) for line in data]\n",
    "    #If we have the labels\n",
    "    if(case):\n",
    "        sent = [word_tokenize(line[1:]) for line in data_cleaned if len(line) > 0]\n",
    "        labels = [int(line[0]) for line in data_cleaned if len(line) > 0] \n",
    "        f.close()\n",
    "        return sent,labels\n",
    "    else:\n",
    "        sent = [word_tokenize(line) for line in data_cleaned]\n",
    "        f.close()\n",
    "        return sent\n",
    "    \n",
    "file_name_train = \"/Users/notebooks_deepl/nlp_project/data/SST/stsa.fine.train.csv\"\n",
    "file_name_test = \"/Users/notebooks_deepl/nlp_project/data/SST/stsa.fine.test.X.csv\"\n",
    "file_name_dev = \"/Users/notebooks_deepl/nlp_project/data/SST/stsa.fine.dev.csv\"\n",
    "\n",
    "sent_train, labels_train = data_pp_1(file_name_train,True)\n",
    "sent_dev, labels_dev = data_pp_1(file_name_dev,True)\n",
    "sent_test = data_pp_1(file_name_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=150000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "sent_emb_train = s2v.encode(sent_train, s2v.build_idf(sent_train))\n",
    "sent_emb_dev = s2v.encode(sent_dev, s2v.build_idf(sent_dev))\n",
    "sent_emb_test = s2v.encode(sent_test, s2v.build_idf(sent_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Train\n",
      "The f1_score is: 0.4709425796983484\n",
      "The accuracy is: 0.49929775280898875\n",
      "--> Test\n",
      "The f1_score is: 0.3674148302479298\n",
      "The accuracy is: 0.3996366939146231\n",
      "With a PCA\n",
      "--> Train\n",
      "The f1_score is: 0.4160345162745509\n",
      "The accuracy is: 0.4610252808988764\n",
      "--> Test\n",
      "The f1_score is: 0.35571935591550596\n",
      "The accuracy is: 0.40236148955495005\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "feat_name = [ \"f_\"+str(i) for i in range(300)]\n",
    "\n",
    "df_train = pd.DataFrame(sent_emb_train, columns = feat_name)\n",
    "df_train[\"class\"] = labels_train\n",
    "df_dev = pd.DataFrame(sent_emb_dev, columns = feat_name)\n",
    "df_dev[\"class\"] = labels_dev\n",
    "df_test = pd.DataFrame(sent_emb_test, columns = feat_name)\n",
    "\n",
    "# Building up the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, Y_train = df_train[feat_name].values, df_train['class'].values\n",
    "\n",
    "clf_LR = LogisticRegression(C = 10, max_iter=10000,solver='lbfgs',multi_class='multinomial', penalty ='l2')\n",
    "\n",
    "clf_LR.fit(X_train, Y_train)\n",
    "\n",
    "print(\"--> Train\")\n",
    "print(\"The f1_score is: \"+str(f1_score(Y_train, clf_LR.predict(X_train), average='macro'))) \n",
    "print(\"The accuracy is: \"+str((accuracy_score(Y_train, clf_LR.predict(X_train))))) \n",
    "\n",
    "X_dev, Y_dev = df_dev[feat_name].values, df_dev['class'].values\n",
    "\n",
    "print(\"--> Test\")\n",
    "print(\"The f1_score is: \"+str(f1_score(Y_dev, clf_LR.predict(X_dev), average='macro'))) \n",
    "print(\"The accuracy is: \"+str((accuracy_score(Y_dev, clf_LR.predict(X_dev)))))\n",
    "\n",
    "\n",
    "print(\"With a PCA\")\n",
    "\n",
    "clf_LR_PCA = LogisticRegression(C = 10, max_iter=10000,solver='lbfgs',multi_class='multinomial', penalty ='l2')\n",
    "\n",
    "pca = PCA(n_components = 120)\n",
    "pca.fit(X_train)\n",
    "X_train_PCA = pca.transform(X_train)\n",
    "\n",
    "\n",
    "clf_LR_PCA.fit(X_train_PCA, Y_train)\n",
    "\n",
    "print(\"--> Train\")\n",
    "print(\"The f1_score is: \"+str(f1_score(Y_train, clf_LR_PCA.predict(X_train_PCA), average='macro'))) \n",
    "print(\"The accuracy is: \"+str((accuracy_score(Y_train, clf_LR_PCA.predict(X_train_PCA))))) \n",
    "\n",
    "X_dev, Y_dev = df_dev[feat_name].values, df_dev['class'].values\n",
    "\n",
    "X_dev_PCA = pca.transform(X_dev)\n",
    "\n",
    "print(\"--> Test\")\n",
    "print(\"The f1_score is: \"+str(f1_score(Y_dev, clf_LR_PCA.predict(X_dev_PCA), average='macro'))) \n",
    "print(\"The accuracy is: \"+str((accuracy_score(Y_dev, clf_LR_PCA.predict(X_dev_PCA)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 1 ... 0 1 3]\n"
     ]
    }
   ],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "X_test = df_train[feat_name].values\n",
    "pred_labels_LR = clf_LR.predict(X_test)\n",
    "print(pred_labels_LR)\n",
    "\n",
    "file_out_LR = open(\"logistic_regression_predictions.txt\",\"w+\")\n",
    "for i in range(len(pred_labels_LR)):\n",
    "    file_out_LR.write(str(pred_labels_LR[i])+\"\\n\")  \n",
    "    \n",
    "file_out_LR.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = os.getcwd()+\"\\data\"\n",
    "\n",
    "def data_pp_2(fname):\n",
    "    f = open(fname,'r')\n",
    "    text = f.read()\n",
    "    data = text.split('\\n')\n",
    "    f.close()\n",
    "    return data\n",
    "    \n",
    "file_name_train = \"/Users/notebooks_deepl/nlp_project/data/SST/stsa.fine.train.csv\"\n",
    "file_name_test = \"/Users/notebooks_deepl/nlp_project/data/SST/stsa.fine.test.X.csv\"\n",
    "file_name_dev = \"/Users/notebooks_deepl/nlp_project/data/SST/stsa.fine.dev.csv\"\n",
    "\n",
    "text_train = data_pp_2(file_name_train)\n",
    "text_dev = data_pp_2(file_name_dev)\n",
    "text_test = data_pp_2(file_name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "n = 16000\n",
    "\n",
    "def text_to_2Dint(text,n):\n",
    "    in_pad = []\n",
    "    labels = []\n",
    "    for line in (line for line in text if len(line) > 1):\n",
    "        in_pad.append(keras.preprocessing.text.one_hot(line[1:], n = n*1.3,filters=string.punctuation, lower=True, split=' '))\n",
    "        labels.append(int(line[0]))\n",
    "    return in_pad,labels\n",
    "\n",
    "def text_to_2Dint_test(text,n):\n",
    "    in_pad = []\n",
    "    for line in (line for line in text if len(line) > 1):\n",
    "        in_pad.append(keras.preprocessing.text.one_hot(line, n = n*1.3,filters=string.punctuation, lower=True, split=' '))\n",
    "    return in_pad\n",
    "\n",
    "in_pad_train,labels_train = text_to_2Dint(text_train,n)\n",
    "in_pad_dev,labels_dev = text_to_2Dint(text_dev,n)\n",
    "in_pad_test = text_to_2Dint_test(text_test,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "X_train_PAD = keras.preprocessing.sequence.pad_sequences(in_pad_train, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "X_dev_PAD = keras.preprocessing.sequence.pad_sequences(in_pad_dev, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)\n",
    "X_test_PAD = keras.preprocessing.sequence.pad_sequences(in_pad_test, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(15, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "embed_dim  = 50  # word embedding dimension\n",
    "nhid       = 15  # number of hidden units in the LSTM\n",
    "vocab_size = int(n*1.3)  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          1040000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 15)                3960      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 1,044,040\n",
      "Trainable params: 1,044,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'RMSProp' # find the right optimizer #RMSProp\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 16s 2ms/step - loss: 1.5732 - acc: 0.2860 - val_loss: 1.5622 - val_acc: 0.2779\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 13s 2ms/step - loss: 1.5173 - acc: 0.3460 - val_loss: 1.5020 - val_acc: 0.3460\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 13s 1ms/step - loss: 1.3969 - acc: 0.3977 - val_loss: 1.4226 - val_acc: 0.3787\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 13s 2ms/step - loss: 1.2629 - acc: 0.4748 - val_loss: 1.3955 - val_acc: 0.3951\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 15s 2ms/step - loss: 1.1504 - acc: 0.5414 - val_loss: 1.3888 - val_acc: 0.3942\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 14s 2ms/step - loss: 1.0487 - acc: 0.5943 - val_loss: 1.4128 - val_acc: 0.3942\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "Y_train_categorical = np_utils.to_categorical(labels_train,5)\n",
    "Y_dev_categorical = np_utils.to_categorical(labels_dev,5)\n",
    "history = model.fit(X_train_PAD, Y_train_categorical, batch_size=bs, epochs=n_epochs, validation_data=(X_dev_PAD, Y_dev_categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x147e50e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend([\"train\",\"dev\"],loc ='upper left')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Loss function\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"train\",\"dev\"], loc ='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "predictions = model.predict(X_test_PAD)\n",
    "\n",
    "file = open(\"LSTM_predictions.txt\",\"w+\")\n",
    "for line in predictions:\n",
    "    file.write(str(np.argmax(line))+\"\\n\")  \n",
    "    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
